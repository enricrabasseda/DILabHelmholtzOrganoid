{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics on test dataset\n",
    "\n",
    "In this notebook we evaluate all the different models trained previously and compare their metrics on a test dataset.\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/ubuntu/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "First we will load the test dataset from the small subset of the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test dataset: 379\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('json', data_files='/home/ubuntu/data/small_metadata.json')\n",
    "\n",
    "test_dataset = ds[\"train\"].filter(lambda example: example[\"split\"] == \"test\")\n",
    "del(ds)\n",
    "print(f'Length of test dataset:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a SAM dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sam_dataset import SAMDataset\n",
    "from transformers import SamProcessor\n",
    "# Define dataset location folder\n",
    "data_folder = \"/home/ubuntu/data/\"\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "sam_test_dataset = SAMDataset(dataset=test_dataset, processor=processor, data_folder=data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models\n",
    "\n",
    "We load all the models trained before.\n",
    "* Model 1: SAM trained with topological and geometrical loss with box prompt. Best one is for $\\lambda = 0.1$.\n",
    "* Model 2: SAM trained with topological and geometrical loss with point prompt\n",
    "* Model 3: SAM trained with geometrical loss with box prompt\n",
    "* Model 4: SAM trained with geometrical loss with point prompt\n",
    "* Model 5: MedSAM with box prompt\n",
    "* Model 6: SAM ViT-base model with box prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel\n",
    "\n",
    "model_1 = torch.load(\"/home/ubuntu/models/sam_experiments/topo-no-reg-lambda-0.1_geom-int150_box-prompt.pth\")\n",
    "for name, param in model_1.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "model_2 = torch.load(\"/home/ubuntu/models/sam_experiments/small_private/topo-no-reg_geom-int150_box-prompt.pth\")\n",
    "for name, param in model_2.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "model_3 = torch.load(\"/home/ubuntu/models/sam_experiments/dev280/topo-no-reg_geom-int150_box-prompt.pth\")\n",
    "for name, param in model_3.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "model_4 = torch.load(\"/home/ubuntu/models/sam_experiments/no-topo_geom-int150_box-prompt.pth\")\n",
    "for name, param in model_4.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "model_medsam = SamModel.from_pretrained(\"wanglab/medsam-vit-base\")\n",
    "for name, param in model_medsam.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "model_sam_base = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "for name, param in model_sam_base.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the metrics for every model\n",
    "\n",
    "Given a model and a dataloader this function returns a dictionary with all the metrics for that model.\n",
    "\n",
    "We will compute many different metrics for every model:\n",
    "| Metric     | Definition |\n",
    "| ----- | ---------- |\n",
    "| Intersection over Union `IoU`   | Formula = Intersection / Union |\n",
    "| Dice Square Coefficient `DSC` | Formula: Dice = (2 * Intersection) / (Sum of squares of the masks' areas) |\n",
    "| Surface Distance `SurfDist` | Recently discovered by Deepmind |\n",
    "| Sensitivity `Sens` | Formula: Sensitivity = True Positives / (True Positives + False Negatives) |\n",
    "| Specificity `Spec` | Formula: Specificity = True Negatives / (True Negatives + False Positives) |\n",
    "| Hausdorff Distance `HausDist` | Quantifies the maximum distance between the contours of the predicted and ground truth masks |\n",
    "| Average Precision `AP` | Computed by integrating the precision-recall curve |\n",
    "| F1 Score `F1` | Formula: F1 Score = 2 * (Precision * Recall) / (Precision + Recall) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from utils.metrics import iou, dsc, surfdist, sensitivity, specificity, hausdorff_dist, ap, f1_score\n",
    "from statistics import mean\n",
    "def metrics_calculation(model, test_dataloader, prompt, device, noised_prompt):\n",
    "    \"\"\"\n",
    "    Compute all the metrics above for one model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.model): model to use\n",
    "        test_dataloader (torch.utils.data.dataloader.DataLoader): dataloader for the test dataset\n",
    "        prompt (string): \"box\" or \"point\" depending on how we want to do the inference\n",
    "        device (string): device where we work\n",
    "        noised_prompt (Bool): denoise prompt\n",
    "\n",
    "    Returns:\n",
    "        metrics_dict (dictionary): dictionary containing all array with metrics for every image\n",
    "    \"\"\"\n",
    "    iou_values = []\n",
    "    dsc_values = []\n",
    "    # surfdist_values = []\n",
    "    sens_values = []\n",
    "    spec_values = []\n",
    "    # hausdist_values = []\n",
    "    ap_values = []\n",
    "    f1_values = []\n",
    "\n",
    "    # Set model to device\n",
    "    model.to(device)\n",
    "    # Set evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Run on all batches\n",
    "    for batch in test_dataloader:\n",
    "        # Get ground-truth masks\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().unsqueeze(1).to(device)\n",
    "        _, _, m_h, m_w = ground_truth_masks.shape\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if noised_prompt:\n",
    "                # Forward pass\n",
    "                if prompt == \"point\":\n",
    "                    outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                                    input_points=batch[\"input_points\"].to(device),\n",
    "                                    multimask_output=False)\n",
    "                elif prompt == \"box\":\n",
    "                    outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                                    input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                                    multimask_output=False)\n",
    "            elif not noised_prompt: \n",
    "                # Get middle point\n",
    "                x = (batch[\"gt_box\"][0] + batch[\"gt_box\"][2])/2\n",
    "                y = (batch[\"gt_box\"][1] + batch[\"gt_box\"][3])/2\n",
    "\n",
    "                # Compute the inputs for the desired image\n",
    "                inputs = processor(batch[\"original_image\"], input_boxes=[[batch[\"gt_box\"]]], \n",
    "                                input_points=[[[x.item(), y.item()]]], return_tensors=\"pt\")  \n",
    "\n",
    "                # Forward pass\n",
    "                if prompt == \"point\":\n",
    "                    outputs = model(pixel_values=inputs[\"pixel_values\"].to(device),\n",
    "                                    input_points=inputs[\"input_points\"].to(device),\n",
    "                                    multimask_output=False)\n",
    "                elif prompt == \"box\":\n",
    "                    outputs = model(pixel_values=inputs[\"pixel_values\"].to(device),\n",
    "                                    input_boxes=inputs[\"input_boxes\"].to(device),\n",
    "                                    multimask_output=False)\n",
    "            # Get masks\n",
    "            predicted_masks = outputs.pred_masks.to(device)\n",
    "            # Masks post processing\n",
    "            predicted_masks = F.interpolate(predicted_masks.squeeze(1), (1024, 1024), \n",
    "                                            mode=\"bilinear\", align_corners=False)\n",
    "            predicted_masks = predicted_masks[..., :992, :1024]\n",
    "            predicted_masks = F.interpolate(predicted_masks, (m_h, m_w), \n",
    "                                            mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            # Apply sigmoid\n",
    "            predicted_masks = torch.sigmoid(predicted_masks)\n",
    "\n",
    "            # Compute metrics\n",
    "            iou_values.append(iou(predicted_masks, ground_truth_masks))\n",
    "            dsc_values.append(dsc(predicted_masks, ground_truth_masks))\n",
    "            # surfdist_values.append(surfdist(predicted_masks, ground_truth_masks))\n",
    "            sens_values.append(sensitivity(predicted_masks, ground_truth_masks))\n",
    "            spec_values.append(specificity(predicted_masks, ground_truth_masks))\n",
    "            # hausdist_values.append(hausdorff_dist(predicted_masks, ground_truth_masks))\n",
    "            ap_values.append(ap(predicted_masks, ground_truth_masks))\n",
    "            f1_values.append(f1_score(predicted_masks, ground_truth_masks))\n",
    "    \n",
    "    # Create dictionary with all values\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[\"IoU\"] = mean(iou_values)\n",
    "    metrics_dict[\"DSC\"] = mean(dsc_values)\n",
    "    # metrics_dict[\"SurfDist\"] = mean(surfdist_values)\n",
    "    metrics_dict[\"Sens\"] = mean(sens_values)\n",
    "    metrics_dict[\"Spec\"] = mean(spec_values)\n",
    "    # metrics_dict[\"HausDist\"] = mean(hausdist_values)\n",
    "    metrics_dict[\"AP\"] = mean(ap_values)\n",
    "    metrics_dict[\"F1\"] = mean(f1_values)\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as DataLoader\n",
    "\n",
    "# Create dataloader for the dataset\n",
    "sam_test_dataloader = DataLoader.DataLoader(sam_test_dataset, batch_size=1, shuffle=True)\n",
    "device = \"cuda\"\n",
    "\n",
    "# Compute metrics\n",
    "metrics_model_1 = metrics_calculation(model = model_1.to(device), test_dataloader=sam_test_dataloader, prompt = \"box\", device = \"cuda\", noised_prompt = False)\n",
    "metrics_model_2 = metrics_calculation(model = model_2.to(device), test_dataloader=sam_test_dataloader, prompt = \"point\", device = \"cuda\", noised_prompt = False)\n",
    "metrics_model_3 = metrics_calculation(model = model_3.to(device), test_dataloader=sam_test_dataloader, prompt = \"box\", device = \"cuda\", noised_prompt = False)\n",
    "metrics_model_4 = metrics_calculation(model = model_4.to(device), test_dataloader=sam_test_dataloader, prompt = \"point\", device = \"cuda\", noised_prompt = False)\n",
    "metrics_medsam = metrics_calculation(model = model_medsam.to(device), test_dataloader=sam_test_dataloader, prompt = \"box\", device = \"cuda\", noised_prompt = False)\n",
    "metrics_sam_base = metrics_calculation(model = model_sam_base.to(device), test_dataloader=sam_test_dataloader, prompt = \"box\", device = \"cuda\", noised_prompt = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results.\n",
    "\n",
    "We will save the results on a `models_metrics_complete.csv` that can be later used to compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save information in lists\n",
    "models_list = [\"SAM trained with Topo. + Geom. Loss and box prompt on private and intestinal dataset\",\n",
    "               \"SAM trained with Topo. + Geom. Loss and box prompt on private dataset\",\n",
    "               \"SAM trained with Topo. + Geom. Loss and box prompt on private dev280 dataset\",\n",
    "               \"SAM trained with Geom. Loss and box prompt on private and intestinal dataset\",\n",
    "               \"MedSAM\",\n",
    "               \"SAM Base\"]\n",
    "iou_list = [metrics_model_1[\"IoU\"], metrics_model_2[\"IoU\"], metrics_model_3[\"IoU\"], metrics_model_4[\"IoU\"], metrics_medsam[\"IoU\"], metrics_sam_base[\"IoU\"]]\n",
    "dsc_list = [metrics_model_1[\"DSC\"], metrics_model_2[\"DSC\"], metrics_model_3[\"DSC\"], metrics_model_4[\"DSC\"], metrics_medsam[\"DSC\"], metrics_sam_base[\"DSC\"]]\n",
    "# surfdist_list = [metrics_model_1[\"SurfDist\"], metrics_model_2[\"SurfDist\"], metrics_model_3[\"SurfDist\"], metrics_model_4[\"SurfDist\"]]\n",
    "sens_list = [metrics_model_1[\"Sens\"], metrics_model_2[\"Sens\"], metrics_model_3[\"Sens\"], metrics_model_4[\"Sens\"], metrics_medsam[\"Sens\"], metrics_sam_base[\"Sens\"]]\n",
    "spec_list = [metrics_model_1[\"Spec\"], metrics_model_2[\"Spec\"], metrics_model_3[\"Spec\"], metrics_model_4[\"Spec\"], metrics_medsam[\"Spec\"], metrics_sam_base[\"Spec\"]]\n",
    "# hausdist_list = [metrics_model_1[\"HausDist\"], metrics_model_2[\"HausDist\"], metrics_model_3[\"HausDist\"], metrics_model_4[\"HausDist\"]]\n",
    "ap_list = [metrics_model_1[\"AP\"], metrics_model_2[\"AP\"], metrics_model_3[\"AP\"], metrics_model_4[\"AP\"], metrics_medsam[\"AP\"], metrics_sam_base[\"AP\"]]\n",
    "f1_list = [metrics_model_1[\"F1\"], metrics_model_2[\"F1\"], metrics_model_3[\"F1\"], metrics_model_4[\"F1\"], metrics_medsam[\"F1\"], metrics_sam_base[\"F1\"]]\n",
    "\n",
    "# Save dataframe and later to .csv\n",
    "df = pd.DataFrame(list(zip(models_list, iou_list, dsc_list, \n",
    "                           # surfdist_list,\n",
    "                           sens_list, spec_list, \n",
    "                           # hausdist_list, \n",
    "                           ap_list, f1_list)),\n",
    "                           columns =['Model', 'IoU', 'DSC', \n",
    "                                     # 'Surface Distance', \n",
    "                                     'Sensitivity', 'Specificity',\n",
    "                                     # 'Hausdorff Distance', \n",
    "                                     'AP', 'F1'])\n",
    "\n",
    "df.to_csv(\"/home/ubuntu/models_metrics_complete_.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
