{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orgaquant dataset creation\n",
    "\n",
    "This notebook gives a schema and the total steps to generate the ground-truth masks for the organoids in Orgaquant paper. This dataset contains instestinal organoid pictures, but is designed for object detection tasks. Here we generate a dataset for instance segmentation.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Grounding DINO\n",
    "from groundingdino.util.inference import load_image\n",
    "from groundingdino.util import box_ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore possible warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress SettingWithCopyWarning\n",
    "warnings.filterwarnings(\"ignore\", category=pd.core.generic.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the main directory and the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory as the main directory\n",
    "os.chdir(\"/home/ubuntu/\")\n",
    "# Data directory\n",
    "data_dir = \"/home/ubuntu/data/intestinal_organoid_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use CUDA if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the lists containing all the information:\n",
    "* Directory list: contains the paths to all images.\n",
    "* Boxes list: contains the boxes corresponding to organoids for all images.\n",
    "* Masks list: contains the path to the masks generated by SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_source_list = []\n",
    "boxes_list = []\n",
    "masks_list = []\n",
    "split_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamProcessor\n",
    "from utils.inference_sam import sam_inference_from_dino\n",
    "\n",
    "# Use large encoder here\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-large\").to(device)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset creation\n",
    "\n",
    "In this part the train dataset is generated with the boxes given in `train_labels.csv`.\n",
    "\n",
    "### 1. Get the information from the .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13004, 7)\n"
     ]
    }
   ],
   "source": [
    "df_boxes_train = pd.read_csv(os.path.join(data_dir, \"train_labels.csv\"),\n",
    "                             header = None,\n",
    "                             names= ['image_path', 'x1', 'y1', 'x2', 'y2', 'class_name'])\n",
    "# The name of each image is saved in a new column of the dataframe.\n",
    "df_boxes_train[['image_name']] = df_boxes_train['image_path'].apply(lambda x: pd.Series(os.path.basename(x)))\n",
    "print(df_boxes_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lists for train dataset\n",
    "\n",
    "Create the lists that contain train dataset information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_source_list = []\n",
    "train_boxes_list = []\n",
    "train_masks_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate the masks for every image\n",
    "\n",
    "Given the boxes contained in `train_labels.csv` generate the masks with SAM.\n",
    "\n",
    "We create a list containing all the image names. For each image name we get all the boxes in the dataframe. Then, we run SAM model with the boxes as prompts and save each mask separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_img_names = np.unique(np.array(df_boxes_train['image_name'].tolist()))\n",
    "for i, name in enumerate(unique_img_names):\n",
    "    # Save image path\n",
    "    img_path = os.path.join(data_dir, \"train\", \"images\", name)\n",
    "    train_img_source_list.append(img_path)\n",
    "    img_source_list.append(img_path)\n",
    "    # Save split in dataset\n",
    "    split_list.append(\"train\")\n",
    "    # Save image name root\n",
    "    root, _ = os.path.splitext(name)\n",
    "\n",
    "    # Load the image\n",
    "    image_source, _ = load_image(img_path)\n",
    "    H, W, _ = image_source.shape\n",
    "\n",
    "    # Get all boxes corresponding to the image\n",
    "    df_image = df_boxes_train.query(\"image_name == @name\")\n",
    "    df_image['box'] = df_image.apply(lambda row: np.array([row['x1'], row['y1'], row['x2'], row['y2']]), axis=1)\n",
    "    image_boxes = np.array(df_image['box'].tolist())\n",
    "    train_boxes_list.append(image_boxes)    \n",
    "    boxes_list.append(image_boxes)\n",
    "    # Convert boxes to SAM format\n",
    "    sam_boxes = box_ops.box_xyxy_to_cxcywh(torch.Tensor(image_boxes) / np.array([W, H, W, H]))\n",
    "    \n",
    "    # Get masks with SAM\n",
    "    masks, _, _ = sam_inference_from_dino(image_source, sam_boxes, \n",
    "                                          model, processor, device)\n",
    "    # Transform masks to numpy array of shape [n_masks, width, height]\n",
    "    np_masks = (masks[0].numpy())[:,0,:,:]*1\n",
    "    # Save the masks individually and save the path of each mask\n",
    "    count = 0\n",
    "    image_masks = []\n",
    "    for index, mask in enumerate(np_masks):\n",
    "        # Save the mask as an image\n",
    "        if count < 10:\n",
    "            cv2.imwrite(os.path.join(data_dir, \"train\", \"masks\", root + \"_\" + str(0) + str(count) + \".png\"), mask * 255)\n",
    "        elif count >= 10:\n",
    "            cv2.imwrite(os.path.join(data_dir, \"train\", \"masks\", root + \"_\" + str(count) + \".png\"), mask * 255)\n",
    "        # Save the mask location \n",
    "        if count < 10:\n",
    "            image_masks.append(os.path.join(data_dir, \"train\", \"masks\", root + \"_\" + str(0) + str(count) + \".png\")) \n",
    "        elif count >= 10:\n",
    "            image_masks.append(os.path.join(data_dir, \"train\", \"masks\", root + \"_\" + str(count) + \".png\")) \n",
    "        # Update count\n",
    "        count += 1\n",
    "    masks_list.append(image_masks)    \n",
    "    train_masks_list.append(image_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get dataset dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATASET\n",
      "Total number of images: 1630\n",
      "Total number of masks: 13004\n"
     ]
    }
   ],
   "source": [
    "train_images = 0\n",
    "train_masks = 0\n",
    "for i, masks in enumerate(train_masks_list):\n",
    "    if len(masks) > 0:\n",
    "        train_images += 1\n",
    "        train_masks += len(masks)\n",
    "\n",
    "print(\"TRAIN DATASET\")\n",
    "print(\"Total number of images:\", train_images)\n",
    "print(\"Total number of masks:\", train_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset creation\n",
    "\n",
    "In this part the test dataset is generated with the boxes given in `test_labels.csv`.\n",
    "\n",
    "### 1. Get the information from the .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1135, 7)\n"
     ]
    }
   ],
   "source": [
    "df_boxes_test = pd.read_csv(os.path.join(data_dir, \"test_labels.csv\"))\n",
    "# The name of each image is saved in a new column of the dataframe.\n",
    "df_boxes_test[['image_name']] = df_boxes_test['image_path'].apply(lambda x: pd.Series(os.path.basename(x)))\n",
    "print(df_boxes_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lists for test dataset\n",
    "\n",
    "Create the lists that contain test dataset information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_source_list = []\n",
    "test_boxes_list = []\n",
    "test_masks_list = []\n",
    "test_split = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate the masks for every image\n",
    "\n",
    "Given the boxes contained in `test_labels.csv` generate the masks with SAM.\n",
    "\n",
    "We create a list containing all the image names. For each image name we get all the boxes in the dataframe. Then, we run SAM model with the boxes as prompts and save each mask separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_img_names = np.unique(np.array(df_boxes_test['image_name'].tolist()))\n",
    "for i, name in enumerate(unique_img_names):\n",
    "    # Save image path\n",
    "    img_path = os.path.join(data_dir, \"test\", \"images\", name)\n",
    "    test_img_source_list.append(img_path)\n",
    "    img_source_list.append(img_path)\n",
    "    # Save split in dataset\n",
    "    split_list.append(\"test\")\n",
    "    test_split.append(\"test\")\n",
    "    # Save image name root\n",
    "    root, _ = os.path.splitext(name)\n",
    "\n",
    "    # Load the image\n",
    "    image_source, _ = load_image(img_path)\n",
    "    H, W, _ = image_source.shape\n",
    "\n",
    "    # Get all boxes corresponding to the image\n",
    "    df_image = df_boxes_test.query(\"image_name == @name\")\n",
    "    df_image['box'] = df_image.apply(lambda row: np.array([row['x1'], row['y1'], row['x2'], row['y2']]), axis=1)\n",
    "    image_boxes = np.array(df_image['box'].tolist())\n",
    "    test_boxes_list.append(image_boxes)    \n",
    "    boxes_list.append(image_boxes)\n",
    "    # Convert boxes to SAM format\n",
    "    sam_boxes = box_ops.box_xyxy_to_cxcywh(torch.Tensor(image_boxes) / np.array([W, H, W, H]))\n",
    "    \n",
    "    # Get masks with SAM\n",
    "    masks, _, _ = sam_inference_from_dino(image_source, sam_boxes, \n",
    "                                          model, processor, device)\n",
    "    # Transform masks to numpy array of shape [n_masks, width, height]\n",
    "    np_masks = (masks[0].numpy())[:,0,:,:]*1\n",
    "    # Save the masks individually and save the path of each mask\n",
    "    count = 0\n",
    "    image_masks = []\n",
    "    for index, mask in enumerate(np_masks):\n",
    "        # Save the mask as an image\n",
    "        if count < 10:\n",
    "            cv2.imwrite(os.path.join(data_dir, \"test\", \"masks\", root + \"_\" + str(0) + str(count) + \".png\"), mask * 255)\n",
    "        elif count >= 10:\n",
    "            cv2.imwrite(os.path.join(data_dir, \"test\", \"masks\", root + \"_\" + str(count) + \".png\"), mask * 255)\n",
    "        # Save the mask location \n",
    "        if count < 10:\n",
    "            image_masks.append(os.path.join(data_dir, \"test\", \"masks\", root + \"_\" + str(0) + str(count) + \".png\")) \n",
    "        elif count >= 10:\n",
    "            image_masks.append(os.path.join(data_dir, \"test\", \"masks\", root + \"_\" + str(count) + \".png\")) \n",
    "        # Update count\n",
    "        count += 1\n",
    "    masks_list.append(image_masks)    \n",
    "    test_masks_list.append(image_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get dataset dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATASET\n",
      "Total number of images: 112\n",
      "Total number of masks: 1135\n"
     ]
    }
   ],
   "source": [
    "test_images = 0\n",
    "test_masks = 0\n",
    "for i, masks in enumerate(test_masks_list):\n",
    "    if len(masks) > 0:\n",
    "        test_images += 1\n",
    "        test_masks += len(masks)\n",
    "\n",
    "print(\"TEST DATASET\")\n",
    "print(\"Total number of images:\", test_images)\n",
    "print(\"Total number of masks:\", test_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL: Dataset creation\n",
    "\n",
    "Now we can create the file needed to later load the information as a dataset. To do it, we create a pandas dataframe that we save later as .json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(img_source_list, boxes_list, masks_list, split_list)),\n",
    "               columns =['img', 'boxes', 'masks', 'split'])\n",
    "\n",
    "df.to_json(data_dir + \"/metadata.json\", orient = \"records\", lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "TOTAL DATASET\n",
      "Total number of images: 1742\n",
      "Total number of masks: 14139\n",
      "------------------\n",
      "TRAIN DATASET\n",
      "Total number of images: 1630\n",
      "Total number of masks: 13004\n",
      "------------------\n",
      "TEST DATASET\n",
      "Total number of images: 112\n",
      "Total number of masks: 1135\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------\")\n",
    "print(\"TOTAL DATASET\")\n",
    "print(\"Total number of images:\", train_images + test_images)\n",
    "print(\"Total number of masks:\", train_masks + test_masks)\n",
    "print(\"------------------\")\n",
    "print(\"TRAIN DATASET\")\n",
    "print(\"Total number of images:\", train_images)\n",
    "print(\"Total number of masks:\", train_masks)\n",
    "print(\"------------------\")\n",
    "print(\"TEST DATASET\")\n",
    "print(\"Total number of images:\", test_images)\n",
    "print(\"Total number of masks:\", test_masks)\n",
    "print(\"------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
