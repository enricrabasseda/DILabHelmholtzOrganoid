{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data aggregation\n",
    "\n",
    "This notebook helps to aggregate all data from all datasets in a unique .json file to train SAM on all the data we have collected. We will also generate a smaller dataset for testing implementations.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the main directory and the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory as the main directory\n",
    "os.chdir(\"/home/ubuntu/\")\n",
    "# Data directory\n",
    "data_dir = \"/home/ubuntu/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use CUDA if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private dataset loading\n",
    "\n",
    "Here we save all the metadata of the private dataset on lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"private/metadata.json\", 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "df = pd.concat([pd.json_normalize(entry) for entry in data], ignore_index=True)\n",
    "\n",
    "private_img_source_list = list(df['img'])\n",
    "private_boxes_list = list(df['box'])\n",
    "private_masks_list = list(df['mask'])\n",
    "private_split_list = list(df[\"split\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25962 25962 25962 25962\n"
     ]
    }
   ],
   "source": [
    "print(len(private_img_source_list), len(private_boxes_list), len(private_masks_list), len(private_split_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intestinal organoid dataset loading\n",
    "\n",
    "Here we save all the metadata of the private dataset on lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"intestinal_organoid_dataset/metadata.json\", 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "df = pd.concat([pd.json_normalize(entry) for entry in data], ignore_index=True)\n",
    "\n",
    "intestinalorg_img_source_list = list(df['img'])\n",
    "intestinalorg_boxes_list = list(df['box'])\n",
    "intestinalorg_masks_list = list(df['mask'])\n",
    "intestinalorg_split_list = list(df[\"split\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14139 14139 14139 14139\n"
     ]
    }
   ],
   "source": [
    "print(len(intestinalorg_img_source_list), len(intestinalorg_boxes_list), len(intestinalorg_masks_list), len(intestinalorg_split_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify paths of every dataset\n",
    "\n",
    "Since the aggregated resulting `metadata.json` file will be located in the `/data/` folder, we need to complement the paths appearing in the elements `img` and `mask` adding the prefix of the dataset to which the element belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(private_img_source_list)):\n",
    "    private_img_source_list[i] = \"private/\" + private_img_source_list[i]\n",
    "    private_masks_list[i] = \"private/\" + private_masks_list[i]\n",
    "\n",
    "for i in range(len(intestinalorg_img_source_list)):\n",
    "    intestinalorg_img_source_list[i] = \"intestinal_organoid_dataset/\" + intestinalorg_img_source_list[i]\n",
    "    intestinalorg_masks_list[i] = \"intestinal_organoid_dataset/\" +  intestinalorg_masks_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get smaller dataset\n",
    "\n",
    "Here we will get a smaller dataset containing 10% of the real data for experiments.\n",
    "\n",
    "### Private dataset\n",
    "\n",
    "We filter the number of samples to 10% randomly of `train` and `val` and `test` splits for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(98)\n",
    "\n",
    "# Filter elements containing \"train\"\n",
    "private_train_indices = [index for index, value in enumerate(private_split_list) if \"train\" in value]\n",
    "\n",
    "# Choose 10% of the filtered \"train\" elements randomly\n",
    "num_samples = int(len(private_train_indices) * 0.1)\n",
    "private_train_indices = random.sample(private_train_indices, num_samples)\n",
    "\n",
    "# Filter elements containing \"val\"\n",
    "private_val_indices = [index for index, value in enumerate(private_split_list) if \"val\" in value]\n",
    "\n",
    "# Choose 10% of the filtered \"val\" elements randomly\n",
    "num_samples = int(len(private_val_indices) * 0.1)\n",
    "private_val_indices = random.sample(private_val_indices, num_samples)\n",
    "\n",
    "# Filter elements containing \"test\"\n",
    "private_test_indices = [index for index, value in enumerate(private_split_list) if \"test\" in value]\n",
    "\n",
    "# Choose 10% of the filtered \"test\" elements randomly\n",
    "num_samples = int(len(private_test_indices) * 0.1)\n",
    "private_test_indices = random.sample(private_test_indices, num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of small private train split: 2079\n",
      "Length of small private validation split: 251\n",
      "Length of small private test split: 266\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of small private train split:', len(private_train_indices))\n",
    "print(f'Length of small private validation split:', len(private_val_indices))\n",
    "print(f'Length of small private test split:', len(private_test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intestinal organoid dataset\n",
    "\n",
    "We filter the number of samples to 10% randomly of `train` and `test` splits for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(98)\n",
    "\n",
    "# Filter elements containing \"train\"\n",
    "intestinal_train_indices = [index for index, value in enumerate(intestinalorg_split_list) if \"train\" in value]\n",
    "\n",
    "# Choose 10% of the filtered \"train\" elements randomly\n",
    "num_samples = int(len(intestinal_train_indices) * 0.1)\n",
    "intestinal_train_indices = random.sample(intestinal_train_indices, num_samples)\n",
    "\n",
    "# Filter elements containing \"test\"\n",
    "intestinal_test_indices = [index for index, value in enumerate(intestinalorg_split_list) if \"test\" in value]\n",
    "\n",
    "# Choose 10% of the filtered \"test\" elements randomly\n",
    "num_samples = int(len(intestinal_test_indices) * 0.1)\n",
    "intestinal_test_indices = random.sample(intestinal_test_indices, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of small private train split: 1300\n",
      "Length of small private test split: 113\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of small private train split:', len(intestinal_train_indices))\n",
    "print(f'Length of small private test split:', len(intestinal_test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset aggregation\n",
    "\n",
    "Finally, we aggregate the datasets creating a big dataset that contains all data from the two original sources; and another smaller dataset that will be used for experiments.\n",
    "\n",
    "### Small dataset aggregation\n",
    "\n",
    "First we add all the data as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset_img_source_list = [private_img_source_list[i] for i in private_train_indices] + [private_img_source_list[i] for i in private_val_indices] + [private_img_source_list[i] for i in private_test_indices] + [intestinalorg_img_source_list[i] for i in intestinal_train_indices] + [intestinalorg_img_source_list[i] for i in intestinal_test_indices]\n",
    "small_dataset_box_list = [private_boxes_list[i] for i in private_train_indices] + [private_boxes_list[i] for i in private_val_indices] + [private_boxes_list[i] for i in private_test_indices] + [intestinalorg_boxes_list[i] for i in intestinal_train_indices] + [intestinalorg_boxes_list[i] for i in intestinal_test_indices]\n",
    "small_dataset_mask_list = [private_masks_list[i] for i in private_train_indices] + [private_masks_list[i] for i in private_val_indices] + [private_masks_list[i] for i in private_test_indices] + [intestinalorg_masks_list[i] for i in intestinal_train_indices] + [intestinalorg_masks_list[i] for i in intestinal_test_indices]\n",
    "small_dataset_split_list = [private_split_list[i] for i in private_train_indices] + [private_split_list[i] for i in private_val_indices] + [private_split_list[i] for i in private_test_indices] + [intestinalorg_split_list[i] for i in intestinal_train_indices] + [intestinalorg_split_list[i] for i in intestinal_test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save everything as a `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(small_dataset_img_source_list, small_dataset_box_list, small_dataset_mask_list, small_dataset_split_list)),\n",
    "               columns =['img', 'box', 'mask', 'split'])\n",
    "\n",
    "df.to_json(data_dir + \"small_metadata.json\", orient = \"records\", lines = True)\n",
    "\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of instances in the small dataset: 4009\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of instances in the small dataset:', len(small_dataset_box_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete dataset aggregation\n",
    "\n",
    "First we save all the data as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset_img_source_list = private_img_source_list + intestinalorg_img_source_list\n",
    "big_dataset_box_list = private_boxes_list + intestinalorg_boxes_list\n",
    "big_dataset_mask_list = private_masks_list + intestinalorg_masks_list\n",
    "big_dataset_split_list = private_split_list + intestinalorg_split_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save everything as `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(big_dataset_img_source_list, big_dataset_box_list, big_dataset_mask_list, big_dataset_split_list)),\n",
    "               columns =['img', 'box', 'mask', 'split'])\n",
    "\n",
    "df.to_json(data_dir + \"metadata.json\", orient = \"records\", lines = True)\n",
    "\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of instances in the complete dataset: 40101\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of instances in the complete dataset:', len(big_dataset_box_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
