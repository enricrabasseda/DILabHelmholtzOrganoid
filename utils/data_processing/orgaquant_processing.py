# Generate the ground-truth masks for the organoids in OrgaQuant paper.

# Import relevant libraries
import os
import numpy as np
import torch
import pandas as pd
import cv2
# Grounding DINO
from groundingdino.util.inference import load_image
from groundingdino.util import box_ops

from transformers import SamModel, SamProcessor
from utils.inference_sam import sam_inference_from_dino

# Ignore possible warnings
import warnings
# Suppress SettingWithCopyWarning
warnings.filterwarnings("ignore", category=pd.core.generic.SettingWithCopyWarning)

# Set working directory as the main directory
os.chdir("...")
# Data directory
data_dir = ".../data/intestinal_organoid_dataset"

# Use CUDA if possible
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

# Initialize the lists containing all the information:
# * Directory list: contains the paths to all images.
img_source_list = []
# * Boxes list: contains the boxes corresponding to organoids for all images.
boxes_list = []
# * Masks list: contains the path to the masks generated by SAM.
masks_list = []
# * Split list: contains the split: train/val/test in which the mask belongs
split_list = []
# Use large encoder here
model = SamModel.from_pretrained("facebook/sam-vit-large").to(device)
processor = SamProcessor.from_pretrained("facebook/sam-vit-large")

# --------------------------------------------------------------------------------

# Train dataset creation

# 1. Get information from train_labels.csv
df_boxes_train = pd.read_csv(os.path.join(data_dir, "train_labels.csv"),
                             header = None,
                             names= ['image_path', 'x1', 'y1', 'x2', 'y2', 'class_name'])
# The name of each image is saved in a new column of the dataframe.
df_boxes_train[['image_name']] = df_boxes_train['image_path'].apply(lambda x: pd.Series(os.path.basename(x)))

# 2. Create lists containing train dataset information
train_img_source_list = []
train_boxes_list = []
train_masks_list = []

# 3. Generate the mask for every box and save it
unique_img_names = np.unique(np.array(df_boxes_train['image_name'].tolist()))
for i, name in enumerate(unique_img_names):
    # Save image path
    img_path = os.path.join(data_dir, "train", "images", name)
    train_img_source_list.append(img_path)
    img_source_list.append(img_path)
    # Save split in dataset
    split_list.append("train")
    # Save image name root
    root, _ = os.path.splitext(name)
    # Load the image
    image_source, _ = load_image(img_path)
    H, W, _ = image_source.shape
    # Get all boxes corresponding to the image
    df_image = df_boxes_train.query("image_name == @name")
    df_image['box'] = df_image.apply(lambda row: np.array([row['x1'], row['y1'], row['x2'], row['y2']]), axis=1)
    image_boxes = np.array(df_image['box'].tolist())
    train_boxes_list.append(image_boxes)    
    boxes_list.append(image_boxes)
    # Convert boxes to SAM format
    sam_boxes = box_ops.box_xyxy_to_cxcywh(torch.Tensor(image_boxes) / np.array([W, H, W, H]))
    # Get masks with SAM
    masks, _, _ = sam_inference_from_dino(image_source, sam_boxes, 
                                          model, processor, device)
    # Transform masks to numpy array of shape [n_masks, width, height]
    np_masks = (masks[0].numpy())[:,0,:,:]*1
    # Save the masks individually and save the path of each mask
    count = 0
    image_masks = []
    for index, mask in enumerate(np_masks):
        # Save the mask as an image
        if count < 10:
            cv2.imwrite(os.path.join(data_dir, "train", "masks", root + "_" + str(0) + str(count) + ".png"), mask * 255)
        elif count >= 10:
            cv2.imwrite(os.path.join(data_dir, "train", "masks", root + "_" + str(count) + ".png"), mask * 255)
        # Save the mask location 
        if count < 10:
            image_masks.append(os.path.join(data_dir, "train", "masks", root + "_" + str(0) + str(count) + ".png")) 
        elif count >= 10:
            image_masks.append(os.path.join(data_dir, "train", "masks", root + "_" + str(count) + ".png")) 
        # Update count
        count += 1
    masks_list.append(image_masks)    
    train_masks_list.append(image_masks)

# 4. Get dataset dimensions
train_images = 0
train_masks = 0
for i, masks in enumerate(train_masks_list):
    if len(masks) > 0:
        train_images += 1
        train_masks += len(masks)

print("TRAIN DATASET")
print("Total number of images:", train_images)
print("Total number of masks:", train_masks)

# --------------------------------------------------------------------------------

# Test dataset creation
    
# 1. Get information from test_labels.csv
df_boxes_test = pd.read_csv(os.path.join(data_dir, "test_labels.csv"))
# The name of each image is saved in a new column of the dataframe.
df_boxes_test[['image_name']] = df_boxes_test['image_path'].apply(lambda x: pd.Series(os.path.basename(x)))

# 2. Create the lists that contain test dataset information.
test_img_source_list = []
test_boxes_list = []
test_masks_list = []
test_split = []

# 3. Generate the mask for every box and save it
unique_img_names = np.unique(np.array(df_boxes_test['image_name'].tolist()))
for i, name in enumerate(unique_img_names):
    # Save image path
    img_path = os.path.join(data_dir, "test", "images", name)
    test_img_source_list.append(img_path)
    img_source_list.append(img_path)
    # Save split in dataset
    split_list.append("test")
    test_split.append("test")
    # Save image name root
    root, _ = os.path.splitext(name)
    # Load the image
    image_source, _ = load_image(img_path)
    H, W, _ = image_source.shape
    # Get all boxes corresponding to the image
    df_image = df_boxes_test.query("image_name == @name")
    df_image['box'] = df_image.apply(lambda row: np.array([row['x1'], row['y1'], row['x2'], row['y2']]), axis=1)
    image_boxes = np.array(df_image['box'].tolist())
    test_boxes_list.append(image_boxes)    
    boxes_list.append(image_boxes)
    # Convert boxes to SAM format
    sam_boxes = box_ops.box_xyxy_to_cxcywh(torch.Tensor(image_boxes) / np.array([W, H, W, H]))
    # Get masks with SAM
    masks, _, _ = sam_inference_from_dino(image_source, sam_boxes, 
                                          model, processor, device)
    # Transform masks to numpy array of shape [n_masks, width, height]
    np_masks = (masks[0].numpy())[:,0,:,:]*1
    # Save the masks individually and save the path of each mask
    count = 0
    image_masks = []
    for index, mask in enumerate(np_masks):
        # Save the mask as an image
        if count < 10:
            cv2.imwrite(os.path.join(data_dir, "test", "masks", root + "_" + str(0) + str(count) + ".png"), mask * 255)
        elif count >= 10:
            cv2.imwrite(os.path.join(data_dir, "test", "masks", root + "_" + str(count) + ".png"), mask * 255)
        # Save the mask location 
        if count < 10:
            image_masks.append(os.path.join(data_dir, "test", "masks", root + "_" + str(0) + str(count) + ".png")) 
        elif count >= 10:
            image_masks.append(os.path.join(data_dir, "test", "masks", root + "_" + str(count) + ".png")) 
        # Update count
        count += 1
    masks_list.append(image_masks)    
    test_masks_list.append(image_masks)

# 4. Get dataset dimensions
test_images = 0
test_masks = 0
for i, masks in enumerate(test_masks_list):
    if len(masks) > 0:
        test_images += 1
        test_masks += len(masks)

print("TEST DATASET")
print("Total number of images:", test_images)
print("Total number of masks:", test_masks)

# --------------------------------------------------------------------------------

# FINAL dataset creation
    
# We will write one line per mask in the final metadata.json file for the dataset
final_img_path_list = []
final_box_list = []
final_mask_list = []
final_split_list = []

for i in range(len(img_source_list)):
    n = len(boxes_list[i])
    # If there is at least one mask for this image save one line per mask
    if n > 0:
        # Do the same for all masks
        for j in range(n):
            # Save image path
            final_img_path_list.append(img_source_list[i])
            # Save split label
            final_split_list.append(split_list[i])
            # Save box
            final_box_list.append(boxes_list[i][j])
            # Save mask path
            final_mask_list.append(masks_list[i][j])

# Save it in the metadata.json file  
df = pd.DataFrame(list(zip(final_img_path_list, final_box_list, final_mask_list, final_split_list)),
               columns =['img', 'box', 'mask', 'split'])

df.to_json(data_dir + "/metadata.json", orient = "records", lines = True)

# Get the size of the final dataset
print("------------------")
print("TOTAL DATASET")
print("Total number of images:", train_images + test_images)
print("Total number of masks:", train_masks + test_masks)
print("------------------")
print("TRAIN DATASET")
print("Total number of images:", train_images)
print("Total number of masks:", train_masks)
print("------------------")
print("TEST DATASET")
print("Total number of images:", test_images)
print("Total number of masks:", test_masks)
print("------------------")